{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "##cookies\n",
    "\n",
    "class Test_selenimu1:\n",
    "    def setup(self):\n",
    "        service = Service(r\"D://chromedriver//chromedriver-win64//chromedriver.exe\")##换成自己的chromedriver路径\n",
    "        self.driver = webdriver.Chrome(service=service)\n",
    "        self.driver.get(r\"https://douban.com\")\n",
    "\n",
    "    @pytest.mark.skip(msg=\"先不用\")\n",
    "    def test_cookies(self):\n",
    "        driver = self.driver\n",
    "        driver.implicitly_wait(60)\n",
    "        iframe = driver.find_element(By.TAG_NAME, \"iframe\")\n",
    "        driver.switch_to.frame(iframe)\n",
    "        driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/ul[1]/li[2]\").click()\n",
    "        driver.find_element(By.ID, \"username\").send_keys(\"***********\")\n",
    "        driver.find_element(By.ID, \"password\").send_keys(\"***********\")\n",
    "        #等待60秒，手动登陆\n",
    "        sleep(60)\n",
    "\n",
    "        cookies = driver.get_cookies()\n",
    "\n",
    "        with open(r\"E://py-practice//cookie.txt\", 'w') as file1:\n",
    "            for cookie in cookies:\n",
    "                file1.write(json.dumps(cookie) + \"\\n\")\n",
    "\n",
    "    # @pytest.mark.skip(msg=\"先不用\")\n",
    "    def test_case2(self):\n",
    "        driver = self.driver\n",
    "        driver.get(r\"https://douban.com\")\n",
    "        driver.implicitly_wait(10)\n",
    "        with open(r\"E://py-practice//cookie.txt\", 'r') as file2:\n",
    "            res = file2.readlines()\n",
    "            for line in res:\n",
    "                # print(line)\n",
    "                cookie = json.loads(line)\n",
    "                # print(cookie)\n",
    "                if \"expiry\" in cookie:\n",
    "                    del cookie[\"expiry\"]\n",
    "                driver.add_cookie(cookie)\n",
    "\n",
    "            # for line in file2.readlines():\n",
    "            #     res = line.strip()\n",
    "            #     print(res)\n",
    "            #     mycookie = json.loads(res)\n",
    "            #     print(mycookie)\n",
    "            #     driver.add_cookie(mycookie)\n",
    "             # res = file2.readline()\n",
    "             # mycookies = json.loads(res)\n",
    "        driver.refresh()\n",
    "        sleep(3)\n",
    "    def teardown(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "##运行\n",
    "# 创建类实例并调用方法\n",
    "test_instance = Test_selenimu1()\n",
    "test_instance.setup()\n",
    "test_instance.test_cookies()  # 或 test_instance.test_case2()\n",
    "test_instance.teardown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 禁用安全请求警告\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36',\n",
    "    'cookie': '$$填自己的cookie',\n",
    "}\n",
    "\n",
    "def get_page(i, base_url):\n",
    "    listcnt = 25\n",
    "    start = listcnt * i\n",
    "    params = {\n",
    "        'start': start,\n",
    "        'type': 'new'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=header, params=params, verify=False)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "    except requests.ConnectionError:\n",
    "        print('Error in ', base_url + str(params))\n",
    "        return None\n",
    "\n",
    "def get_discussion_list(base_url):\n",
    "    df = pd.DataFrame(columns=['title', 'elite', 'url', 'author-name', 'author-url', 'r-count', 'time', 'page', 'rank', 'timestamp'])\n",
    "\n",
    "    for i in range(0, 241):##爬取页数\n",
    "        print('page:', i + 1)\n",
    "        res = get_page(i, base_url)\n",
    "        soup = BeautifulSoup(res, 'lxml')\n",
    "        attrs = {'class': 'olt'}\n",
    "        raw_tb = soup.find(attrs=attrs).find_all('tr')\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        for rk, tr in enumerate(raw_tb[1:]):\n",
    "            col = {}\n",
    "            tds = tr.find_all('td')\n",
    "            col['title'] = tds[0].find('a').get('title')\n",
    "            col['elite'] = 1 if tds[0].find(attrs={'class': 'elite_topic_lable'}) else 0\n",
    "            col['url'] = tds[0].find('a').get('href')\n",
    "            col['author-name'] = tds[1].find('a').get_text()\n",
    "            col['author-url'] = tds[1].find('a').get('href')\n",
    "            col['r-count'] = tds[2].get_text()\n",
    "            col['time'] = tds[3].get_text()\n",
    "            col['page'] = i + 1\n",
    "            col['rank'] = rk + 1\n",
    "            col['timestamp'] = timestamp\n",
    "            df = pd.concat([df, pd.DataFrame([col])], ignore_index=True)\n",
    "        sleep(3)\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grouplist_url = '$$填要爬的小组url'\n",
    "    list_df = get_discussion_list(grouplist_url)\n",
    "    list_df.drop_duplicates(subset=['title', 'url'], inplace=True)\n",
    "    list_df.to_csv('E://py-practice//discussion_list.csv', index=False, encoding='utf-8-sig')##保存成文件"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
